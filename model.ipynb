{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"setopatiopinion.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    #get stripped sentences in array removing empty sentences \n",
    "    sentences = re.sub(r'[^क-नःप-रलव-हा-ृेैोौ्ँंॐ‍ अ-ऌएऐओऔ।]', \"\",data)\n",
    "    sentences = sentences.split('।')\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    return sentences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    #get array of tokens/words for each of the sentences\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "\n",
    "        tokenized =  nltk.word_tokenize(sentence)\n",
    "\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    #Combined effect of \n",
    "        # 1. Splitting into sentences \n",
    "        # 2. Tokenizeing sentences\n",
    "    sentences = split_to_sentences(data)\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['विस्तारै', 'त्यसको', 'हिसाब', 'खोज्न', 'थालियो'],\n",
       " ['विस्तारै', 'प्रश्न', 'सोध्न', 'थालियो'],\n",
       " ['विस्तारै', 'उपनिवेशहरू', 'जाग्न', 'थाले']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokenized_data('विस्तारै त्यसको हिसाब खोज्न थालियो। विस्तारै प्रश्न सोध्न थालियो। विस्तारै उपनिवेशहरू जाग्न थाले।')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize sentences and Divide into TEST and TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['पहिला', 'पहिला', 'शक्तिमा', 'हुनेहरूले', 'आफ्नो', 'पोट्रेट', 'बनाउन', 'लगाउने', 'चलन', 'थियो']\n"
     ]
    }
   ],
   "source": [
    "#1. tokenize the data \n",
    "#2. suffle\n",
    "#3. split data into train(80% data) and remaining 20% into test_data\n",
    "tokenized_data = get_tokenized_data(data)\n",
    "print(tokenized_data[0])\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114985 data are split into 91988 train and 22997 test set\n",
      "First training sample:\n",
      "['अर्कातिर', 'दोस्रो', 'विश्वयुद्ध', 'पछि', 'बनेको', 'विश्वव्यवस्थामा', 'परिवर्तन', 'आइरहेको', 'छ']\n",
      "First test sample\n",
      "['यो', 'कर्मकाण्डले', 'नै', 'मुलुकको', 'लोकतान्त्रिक', 'भविष्य', 'निर्धारण', 'गर्छ', 'भन्ने', 'नेहरूको', 'मान्यता', 'छ']\n"
     ]
    }
   ],
   "source": [
    "print(\"{} data are split into {} train and {} test set\".format(\n",
    "    len(tokenized_data), len(train_data), len(test_data)))\n",
    "\n",
    "print(\"First training sample:\")\n",
    "print(train_data[0])\n",
    "      \n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_ftable(tokenized_sentences):\n",
    "   # get dictionary containing all tokens(words) with their frequecy\n",
    "    word_counts = {}\n",
    "    \n",
    "    for sentence in tokenized_sentences: \n",
    "\n",
    "        for token in sentence: \n",
    "\n",
    "\n",
    "            if token not in word_counts :\n",
    "                word_counts[token] = 1\n",
    "            \n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_above_threshold(tokenized_sentences, threshold):\n",
    "    \n",
    "    closed_vocab = []\n",
    "    \n",
    "    # Get the word couts of the tokenized sentences\n",
    "    word_counts = get_words_ftable(tokenized_sentences)\n",
    "    \n",
    "    for word, cnt in word_counts.items(): \n",
    "        \n",
    "        if cnt >= threshold  :\n",
    "            closed_vocab.append(word)\n",
    "\n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    # tokenized sentences with unk\n",
    "    vocabulary = set(vocabulary)\n",
    "    \n",
    "    # Initialize a list that will hold the sentences after less frequent words are replaced by the unknown token\n",
    "    replaced_tokenized_sentences = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        # Initialize the list that will contain a single sentence with \"unknown_token\" replacements\n",
    "        replaced_sentence = []\n",
    "\n",
    "        for token in sentence: \n",
    "\n",
    "            if token in vocabulary: \n",
    "\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "\n",
    "                replaced_sentence.append(unknown_token)\n",
    "     \n",
    "        # Append the list of tokens to the list of lists\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "        \n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, threshold):\n",
    "\n",
    "    # Get the closed vocabulary using the train data\n",
    "    vocabulary = get_vocab_above_threshold (train_data, threshold)\n",
    "    \n",
    "    # For the train data, replace less common words with \"<unk>\"\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token=\"<unk>\")\n",
    "    \n",
    "    # For the test data, replace less common words with \"<unk>\"\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token=\"<unk>\")\n",
    "\n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives vocabulary/ (closed vocabulary with threshold)\n",
    "#      train_data_replaced with <unk> using vocab\n",
    "#      test_data_replaced with <unk> using vocab\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, test_data, threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample:\n",
      "['अर्कातिर', 'दोस्रो', 'विश्वयुद्ध', 'पछि', 'बनेको', '<unk>', 'परिवर्तन', 'आइरहेको', 'छ'] 91988\n",
      "\n",
      "First preprocessed test sample:\n",
      "['यो', '<unk>', 'नै', 'मुलुकको', 'लोकतान्त्रिक', 'भविष्य', 'निर्धारण', 'गर्छ', 'भन्ने', 'नेहरूको', 'मान्यता', 'छ'] 22997\n",
      "\n",
      "First 10 vocabulary:\n",
      "['अर्कातिर', 'दोस्रो', 'विश्वयुद्ध', 'पछि', 'बनेको', 'परिवर्तन', 'आइरहेको', 'छ', 'देवदार', 'पवित्रता']\n",
      "\n",
      "Size of vocabulary: 46638\n"
     ]
    }
   ],
   "source": [
    "print(\"First preprocessed training sample:\")\n",
    "print(train_data_processed[0], len(train_data_processed))\n",
    "print()\n",
    "print(\"First preprocessed test sample:\")\n",
    "print(test_data_processed[0],len(test_data_processed))\n",
    "print()\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(vocabulary[0:10])\n",
    "print()\n",
    "print(\"Size of vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_ftable(tokenized_sentences, n, start_token='<s>', end_token = '<e>'):\n",
    "\n",
    "    # Initialize dictionary of n-grams and their counts\n",
    "    n_grams = {}\n",
    "    \n",
    "    # Go through each sentence in the data\n",
    "    for sentence in tokenized_sentences: \n",
    "        \n",
    "        # prepend start token n times, and  append <e> one time\n",
    "        sentence = n* [start_token] + sentence +  [end_token ]\n",
    "        \n",
    "        # convert list to tuple\n",
    "        # So that the sequence of words can be used as a key in the dictionary\n",
    "        sentence = tuple (sentence)\n",
    "        \n",
    "    #count frequency of each ngram\n",
    "        for i in range(len(sentence) - n + 1 ): \n",
    "\n",
    "            # Get the n-gram from i to i+n\n",
    "            n_gram =   sentence [i: i + n]\n",
    "\n",
    "            if n_gram in n_grams : \n",
    "\n",
    "                n_grams[n_gram] + = 1\n",
    "            else:\n",
    "\n",
    "                n_grams[n_gram] = 1\n",
    "                \n",
    "            #n_grams[n_gram]=n_grams.get(ngram,0)+1\n",
    "    \n",
    "\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, current_ngram_chunk, \n",
    "                         ngram_ftable, nplus1_gram_ftable, vocab_size, k=1.0):\n",
    "    #for finding, given current_chunk what is the probability of the word...\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    current_ngram_chunk = tuple(current_ngram_chunk)\n",
    "    \n",
    "    \n",
    "    current_ngram_freq =  ngram_ftable.get(current_ngram_chunk, 0) \n",
    "    denominator = current_ngram_freq + (k * vocab_size)\n",
    "\n",
    "    # current chunk with word\n",
    "    nplus1_gram_chunk = current_ngram_chunk + (word,)\n",
    "\n",
    "    nplus1_gram_freq = nplus1_gram_ftable.get(nplus1_gram_chunk, 0)\n",
    "        \n",
    "    numerator = nplus1_gram_freq + k\n",
    "\n",
    "    probability = numerator / denominator\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probabilities for all words\n",
    "\n",
    "The function defined below loops over all words in vocabulary to calculate probabilities for all possible words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(current_ngram, ngram_ftable, nplus1_gram_ftable, vocabulary, k=1.0):\n",
    "\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    current_ngram = tuple(current_ngram)\n",
    "    \n",
    "    # add <e> <unk> to the vocabulary\n",
    "    # <s> is not needed since it should not appear as the next word\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, current_ngram, \n",
    "                                           ngram_ftable, nplus1_gram_ftable, \n",
    "                                           vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, ngram_ftable, nplus1_gram_ftable, vocabulary_size,n, k=1.0):\n",
    "   \n",
    "    # prepend <s> and append <e>\n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    # length of sentence (after adding <s> and <e> tokens)\n",
    "    N = len(sentence)\n",
    "    \n",
    "    # The variable p will hold the product\n",
    "    # that is calculated inside the n-root\n",
    "    # Update this in the code below\n",
    "    product_pi = 1.0\n",
    "    \n",
    "    summation=0\n",
    "    # Index t ranges from 0 to N - n, inclusive on both ends\n",
    "    for t in range(0, N-n): # complete this line\n",
    "\n",
    "        # get the n-gram preceding the word at position t\n",
    "        ngram = sentence[t:t+n]\n",
    "        \n",
    "        # get the word at position t\n",
    "        word = sentence[t+n]\n",
    "\n",
    "        probability =estimate_probability(word, ngram, ngram_ftable, nplus1_gram_ftable, vocabulary_size, k=k)\n",
    "        summation+=math.log(probability)\n",
    "#         product_pi *=  (1 / probability)\n",
    "\n",
    "#     perplexity = product_pi ** (1 / N)\n",
    "#     perplexity = math.exp(-summation/ N)\n",
    "\n",
    "    return summation, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_words, ngram_ftable, nplus1_gram_ftable, vocabulary, n, k=1.0, start_with=None):\n",
    "       \n",
    "    \n",
    "    # get the most recent 'n' words from previous words as the previous n-gram\n",
    "    previous_ngram = previous_words[-n:]\n",
    "\n",
    "    # Estimate the probabilities that each word in the vocabulary is the next word\n",
    "    \n",
    "    probabilities = estimate_probabilities(previous_ngram,\n",
    "                                           ngram_ftable, nplus1_gram_ftable,\n",
    "                                           vocabulary, k=k)\n",
    "\n",
    "    suggestion = None\n",
    "    \n",
    "    max_prob = 0\n",
    "\n",
    "    for word, prob in probabilities.items(): \n",
    "        \n",
    "        if start_with is not None:   \n",
    "            if   not word.startswith(start_with):\n",
    "                continue \n",
    "        \n",
    "        if prob > max_prob :\n",
    "            \n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "  \n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_prediction_values(sentence, ngram_ftable, nplus1_gram_ftable, vocabulary,n, k=1.0):\n",
    "   \n",
    "#     # length of previous words\n",
    "    \n",
    "#     # prepend <s> and append <e>\n",
    "#     sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "#     sentence = tuple(sentence)\n",
    "    \n",
    "#     # length of sentence (after adding <s> and <e> tokens)\n",
    "#     N = len(sentence)\n",
    "    \n",
    "#     # The variable p will hold the product\n",
    "#     # that is calculated inside the n-root\n",
    "#     # Update this in the code below\n",
    "#     count = 1    \n",
    "    \n",
    "#     # Index t ranges from 0 to N - n, inclusive on both ends\n",
    "#     for t in range(0, N-n): # complete this line\n",
    "\n",
    "#         # get the n-gram preceding the word at position t\n",
    "#         ngram = sentence[t:t+n]\n",
    "        \n",
    "#         # get the word at position t\n",
    "#         word = sentence[t+n]\n",
    "        \n",
    "\n",
    "#         predicted_word =suggest_a_word(ngram, ngram_ftable, nplus1_gram_ftable, vocabulary, n, k=k)\n",
    "        \n",
    "#         if word==predicted_word:\n",
    "#             count+=1\n",
    "\n",
    "#     total_prediction = N-n+1\n",
    "#     correct_prediction = count\n",
    "    \n",
    "#     return correct_prediction , total_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get multiple suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, ngram_ftable_list, vocabulary, k=1.0, start_with=None):\n",
    "    model_counts = len(previous_tokens)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts):\n",
    "        ngram_ftable = ngram_ftable_list[i]\n",
    "        nplus1_gram_ftable = ngram_ftable_list[i+1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, ngram_ftable,\n",
    "                                    nplus1_gram_ftable, vocabulary,\n",
    "                                    k=k,n=i+1, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggest multiple words using n-grams of varying length\n",
    "\n",
    "Using n-grams of varying lengths (unigrams, bigrams, trigrams, 4-grams,5-grams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('बनेको',), 378)\n",
      "(('पछि', 'बनेको'), 2)\n",
      "(('विश्वयुद्ध', 'पछि', 'बनेको'), 1)\n",
      "(('दोस्रो', 'विश्वयुद्ध', 'पछि', 'बनेको'), 1)\n",
      "(('अर्कातिर', 'दोस्रो', 'विश्वयुद्ध', 'पछि', 'बनेको'), 1)\n"
     ]
    }
   ],
   "source": [
    "ngram_ftable_list = []\n",
    "for n in range(1, 6):\n",
    "    ngram_ftable = get_ngrams_ftable(train_data_processed, n)\n",
    "    print(list(ngram_ftable.items())[5])\n",
    "    ngram_ftable_list.append(ngram_ftable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pp(test_sentences, ngram_ftable_list, vocab_size, k=1.0):\n",
    "    ngram_accuracy_list=[]\n",
    "    for i in range(4):\n",
    "        count=0\n",
    "        total=0\n",
    "        for sentence in test_sentences:\n",
    "            c,t=calculate_perplexity(sentence,ngram_ftable_list[i],ngram_ftable_list[i+1],vocab_size,n=i+1,k=k)\n",
    "            count+=c\n",
    "            total+=t\n",
    "        ngram_accuracy_list.append(math.exp(count/total))\n",
    "    return ngram_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00026906287491775574,\n",
       " 0.00017160021411939935,\n",
       " 0.0002484017651750977,\n",
       " 0.00039325419330332723]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_pp(test_data_processed, ngram_ftable_list, len(vocabulary), k=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_accuracy(test_sentences, ngram_ftable_list, vocabulary, k=1.0):\n",
    "#     ngram_accuracy_list=[]\n",
    "#     for i in range(4):\n",
    "#         count=0\n",
    "#         total=0\n",
    "#         for sentence in test_sentences:\n",
    "#             c,t=calculate_prediction_values(sentence,ngram_ftable_list[i],ngram_ftable_list[i+1],vocabulary,n=i+1,k=k)\n",
    "#             count+=c\n",
    "#             total+=t\n",
    "#         ngram_accuracy_list.append(count/total)\n",
    "#         print(count/total)\n",
    "#     return ngram_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['कम्प्युटर'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('इन्जिनियरिङको', 0.00014994109456999035)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"कम्प्युटर\"]\n",
    "tmp_suggest8 = get_suggestions(previous_tokens, ngram_ftable_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['कालो'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('चारकोलले', 0.0010878375495925942)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"कालो\"]\n",
    "tmp_suggest8 = get_suggestions(previous_tokens, ngram_ftable_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
